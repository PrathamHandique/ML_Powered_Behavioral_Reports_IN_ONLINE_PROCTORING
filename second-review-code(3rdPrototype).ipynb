{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eb1648f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1729866614.685490 59467917 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1729866614.701567 59468843 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1729866614.709347 59468843 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/prathamhandique/Downloads/anaconda3/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Blink Detection Parameters\n",
    "EYE_AR_THRESH = 0.25\n",
    "EYE_AR_CONSEC_FRAMES = 3\n",
    "blink_counter = 0\n",
    "blink_total = 0\n",
    "cheat_intensity = 0\n",
    "voice_detected = False  # To track voice detection\n",
    "\n",
    "# Iris and Eye Landmarks\n",
    "LEFT_EYE = [362, 385, 387, 263, 373, 380]  # Correct landmarks for EAR\n",
    "RIGHT_EYE = [33, 160, 158, 133, 153, 144]  # Correct landmarks for EAR\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "# Lip Landmarks\n",
    "UPPER_LIP_INDEX = 13\n",
    "LOWER_LIP_INDEX = 14\n",
    "lip_open_count = 0\n",
    "lip_open = False\n",
    "\n",
    "# EAR Calculation Function\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Extract Eye Landmarks from Face Mesh\n",
    "def extract_eye_landmarks(landmarks, indices):\n",
    "    return np.array([(landmarks[i].x, landmarks[i].y) for i in indices])\n",
    "\n",
    "# Voice Detection Function\n",
    "def detect_voice():\n",
    "    global voice_detected\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        while True:\n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=2, phrase_time_limit=2)\n",
    "                recognizer.recognize_google(audio)  # Just to ensure there's some sound\n",
    "                voice_detected = True\n",
    "            except sr.WaitTimeoutError:\n",
    "                voice_detected = False\n",
    "            except sr.UnknownValueError:\n",
    "                voice_detected = False\n",
    "\n",
    "# Gaze Direction Utility Functions\n",
    "def get_landmark_coordinates(landmarks, index, image_shape):\n",
    "    x = int(landmarks[index].x * image_shape[1])\n",
    "    y = int(landmarks[index].y * image_shape[0])\n",
    "    return (x, y)\n",
    "\n",
    "def get_gaze_direction(landmarks, image_shape):\n",
    "    left_eye = get_landmark_coordinates(landmarks, 33, image_shape)\n",
    "    right_eye = get_landmark_coordinates(landmarks, 263, image_shape)\n",
    "    nose_tip = get_landmark_coordinates(landmarks, 1, image_shape)\n",
    "    \n",
    "    eye_center = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)\n",
    "    nose_to_eye_vector = np.array([eye_center[0] - nose_tip[0], eye_center[1] - nose_tip[1]])\n",
    "    \n",
    "    if nose_to_eye_vector[0] > 20:\n",
    "        return \"head turned Left\"\n",
    "    elif nose_to_eye_vector[0] < -20:\n",
    "        return \"head turned Right\"\n",
    "    else:\n",
    "        return \"Looking Forward\"\n",
    "\n",
    "# Start voice detection in a separate thread\n",
    "voice_thread = threading.Thread(target=detect_voice, daemon=True)\n",
    "voice_thread.start()\n",
    "\n",
    "# Start webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.5,\n",
    "    min_tracking_confidence=0.5) as face_mesh:\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        img_h, img_w = image.shape[:2]\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "                # Extract eye landmarks\n",
    "                left_eye = extract_eye_landmarks(face_landmarks.landmark, LEFT_EYE)\n",
    "                right_eye = extract_eye_landmarks(face_landmarks.landmark, RIGHT_EYE)\n",
    "\n",
    "                # Calculate EAR for both eyes\n",
    "                left_eye_ear = eye_aspect_ratio(left_eye)\n",
    "                right_eye_ear = eye_aspect_ratio(right_eye)\n",
    "                ear = (left_eye_ear + right_eye_ear) / 2.0\n",
    "\n",
    "                # Detect blink based on EAR threshold\n",
    "                if ear < EYE_AR_THRESH:\n",
    "                    blink_counter += 1\n",
    "                else:\n",
    "                    if blink_counter >= EYE_AR_CONSEC_FRAMES:\n",
    "                        blink_total += 1\n",
    "                        cheat_intensity += 1\n",
    "                    blink_counter = 0\n",
    "\n",
    "                cv2.putText(image, f\"Blinks: {blink_total}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                # Detect gaze direction using nose and eye center\n",
    "                gaze_direction = get_gaze_direction(face_landmarks.landmark, image.shape)\n",
    "                cv2.putText(image, f\"Gaze: {gaze_direction}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "                # Detect lip movement\n",
    "                upper_lip = face_landmarks.landmark[UPPER_LIP_INDEX]\n",
    "                lower_lip = face_landmarks.landmark[LOWER_LIP_INDEX]\n",
    "                lip_distance = abs(upper_lip.y - lower_lip.y)\n",
    "\n",
    "                if lip_distance > 0.05:  # Adjust threshold if needed\n",
    "                    if not lip_open:\n",
    "                        lip_open_count += 1\n",
    "                        lip_open = True\n",
    "                else:\n",
    "                    lip_open = False\n",
    "\n",
    "                cv2.putText(image, f\"Lip Opens: {lip_open_count}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "                # Calculate and display iris positions\n",
    "                mesh_points = np.array([np.multiply([p.x, p.y], [img_w, img_h]).astype(int) for p in face_landmarks.landmark])\n",
    "\n",
    "                (l_cx, l_cy), l_radius = cv2.minEnclosingCircle(mesh_points[LEFT_IRIS])\n",
    "                center_left = np.array([l_cx, l_cy], dtype=np.int32)\n",
    "                cv2.circle(image, center_left, int(l_radius), (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                (r_cx, r_cy), r_radius = cv2.minEnclosingCircle(mesh_points[RIGHT_IRIS])\n",
    "                center_right = np.array([r_cx, r_cy], dtype=np.int32)\n",
    "                cv2.circle(image, center_right, int(r_radius), (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                # Get horizontal position of the iris relative to the eye width\n",
    "                left_eye_width = np.linalg.norm(left_eye[0] - left_eye[3])\n",
    "                right_eye_width = np.linalg.norm(right_eye[0] - right_eye[3])\n",
    "\n",
    "                left_gaze_ratio = (center_left[0] - left_eye[0][0]) / left_eye_width\n",
    "                right_gaze_ratio = (center_right[0] - right_eye[0][0]) / right_eye_width\n",
    "\n",
    "                # Determine the direction of gaze using iris positions\n",
    "                iris_gaze_direction = \"Center\"\n",
    "                if left_gaze_ratio < 0.5 and right_gaze_ratio < 0.5:\n",
    "                    iris_gaze_direction = \"Looking Left\"\n",
    "                elif left_gaze_ratio > 0.5 and right_gaze_ratio > 0.5:\n",
    "                    iris_gaze_direction = \"Looking Right\"\n",
    "\n",
    "                cv2.putText(image, f'Iris Gaze: {iris_gaze_direction}', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Display voice detection status\n",
    "        if voice_detected:\n",
    "            cv2.putText(image, \"Voice Detected!\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        # Display the image\n",
    "        cv2.imshow('Gaze, Blink, Voice, Lip, and Iris Detector', image)\n",
    "\n",
    "        if cv2.waitKey(5) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "cap.release()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5229a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
