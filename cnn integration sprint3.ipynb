{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21dacf29",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1730083533.456030 60480034 gl_context.cc:357] GL version: 2.1 (2.1 Metal - 88), renderer: Apple M1\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "W0000 00:00:1730083533.468834 60480757 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "W0000 00:00:1730083533.474881 60480757 inference_feedback_manager.cc:114] Feedback manager requires a model with a single signature inference. Disabling support for feedback tensors.\n",
      "/Users/prathamhandique/Downloads/anaconda3/lib/python3.10/site-packages/google/protobuf/symbol_database.py:55: UserWarning: SymbolDatabase.GetPrototype() is deprecated. Please use message_factory.GetMessageClass() instead. SymbolDatabase.GetPrototype() will be removed soon.\n",
      "  warnings.warn('SymbolDatabase.GetPrototype() is deprecated. Please '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data saved to suspicious_activity_data.csv\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "from scipy.spatial import distance\n",
    "import speech_recognition as sr\n",
    "import threading\n",
    "import time\n",
    "import psutil\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import csv #edit\n",
    "\n",
    "# Initialize MediaPipe Face Mesh\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "\n",
    "# Blink Detection Parameters\n",
    "EYE_AR_THRESH = 0.25\n",
    "EYE_AR_CONSEC_FRAMES = 3\n",
    "blink_counter = 0\n",
    "blink_total = 0\n",
    "cheat_intensity = 0\n",
    "voice_detected = False  # To track voice detection\n",
    "\n",
    "# Iris and Eye Landmarks\n",
    "LEFT_EYE = [362, 385, 387, 263, 373, 380]\n",
    "RIGHT_EYE = [33, 160, 158, 133, 153, 144]\n",
    "LEFT_IRIS = [474, 475, 476, 477]\n",
    "RIGHT_IRIS = [469, 470, 471, 472]\n",
    "\n",
    "# Lip Landmarks\n",
    "UPPER_LIP_INDEX = 13\n",
    "LOWER_LIP_INDEX = 14\n",
    "lip_open_count = 0\n",
    "lip_open = False\n",
    "\n",
    "# Lists to collect performance data\n",
    "cpu_usage_data = []\n",
    "memory_usage_data = []\n",
    "fps_data = []\n",
    "\n",
    "# Lists to collect event data\n",
    "blink_data = []\n",
    "voice_data = []\n",
    "lip_open_data = []\n",
    "gaze_data = []\n",
    "\n",
    "# EAR Calculation Function\n",
    "def eye_aspect_ratio(eye):\n",
    "    A = distance.euclidean(eye[1], eye[5])\n",
    "    B = distance.euclidean(eye[2], eye[4])\n",
    "    C = distance.euclidean(eye[0], eye[3])\n",
    "    ear = (A + B) / (2.0 * C)\n",
    "    return ear\n",
    "\n",
    "# Extract Eye Landmarks from Face Mesh\n",
    "def extract_eye_landmarks(landmarks, indices):\n",
    "    return np.array([(landmarks[i].x, landmarks[i].y) for i in indices])\n",
    "\n",
    "# Voice Detection Function\n",
    "def detect_voice():\n",
    "    global voice_detected\n",
    "    recognizer = sr.Recognizer()\n",
    "    mic = sr.Microphone()\n",
    "\n",
    "    with mic as source:\n",
    "        recognizer.adjust_for_ambient_noise(source)\n",
    "        while True:\n",
    "            try:\n",
    "                audio = recognizer.listen(source, timeout=2, phrase_time_limit=2)\n",
    "                recognizer.recognize_google(audio)\n",
    "                voice_detected = True\n",
    "            except sr.WaitTimeoutError:\n",
    "                voice_detected = False\n",
    "            except sr.UnknownValueError:\n",
    "                voice_detected = False\n",
    "\n",
    "# Gaze Direction Utility Functions\n",
    "def get_landmark_coordinates(landmarks, index, image_shape):\n",
    "    x = int(landmarks[index].x * image_shape[1])\n",
    "    y = int(landmarks[index].y * image_shape[0])\n",
    "    return (x, y)\n",
    "\n",
    "def get_gaze_direction(landmarks, image_shape):\n",
    "    left_eye = get_landmark_coordinates(landmarks, 33, image_shape)\n",
    "    right_eye = get_landmark_coordinates(landmarks, 263, image_shape)\n",
    "    nose_tip = get_landmark_coordinates(landmarks, 1, image_shape)\n",
    "    \n",
    "    eye_center = ((left_eye[0] + right_eye[0]) // 2, (left_eye[1] + right_eye[1]) // 2)\n",
    "    nose_to_eye_vector = np.array([eye_center[0] - nose_tip[0], eye_center[1] - nose_tip[1]])\n",
    "    \n",
    "    if nose_to_eye_vector[0] > 20:\n",
    "        return \"head turned Left\"\n",
    "    elif nose_to_eye_vector[0] < -20:\n",
    "        return \"head turned Right\"\n",
    "    else:\n",
    "        return \"Looking Forward\"\n",
    "\n",
    "# Start voice detection in a separate thread\n",
    "voice_thread = threading.Thread(target=detect_voice, daemon=True)\n",
    "voice_thread.start()\n",
    "\n",
    "# Start webcam feed\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "start_time = time.time()  # Start time for 10-second capture\n",
    "\n",
    "with mp_face_mesh.FaceMesh(\n",
    "    max_num_faces=1,\n",
    "    refine_landmarks=True,\n",
    "    min_detection_confidence=0.4,\n",
    "    min_tracking_confidence=0.4) as face_mesh:\n",
    "    frame_data = []\n",
    "    while cap.isOpened() and (time.time() - start_time) < 10:  # Capture for 10 seconds\n",
    "        frame_start_time = time.time()  # Start time for frame processing\n",
    "        success, image = cap.read()\n",
    "        if not success:\n",
    "            print(\"Ignoring empty camera frame.\")\n",
    "            continue\n",
    "\n",
    "        image.flags.writeable = False\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        results = face_mesh.process(image)\n",
    "\n",
    "        image.flags.writeable = True\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_RGB2BGR)\n",
    "        img_h, img_w = image.shape[:2]\n",
    "\n",
    "        if results.multi_face_landmarks:\n",
    "            for face_landmarks in results.multi_face_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    image=image,\n",
    "                    landmark_list=face_landmarks,\n",
    "                    connections=mp_face_mesh.FACEMESH_TESSELATION,\n",
    "                    landmark_drawing_spec=None,\n",
    "                    connection_drawing_spec=mp_drawing_styles.get_default_face_mesh_tesselation_style())\n",
    "\n",
    "                # Extract eye landmarks\n",
    "                left_eye = extract_eye_landmarks(face_landmarks.landmark, LEFT_EYE)\n",
    "                right_eye = extract_eye_landmarks(face_landmarks.landmark, RIGHT_EYE)\n",
    "\n",
    "                # Calculate EAR for both eyes\n",
    "                left_eye_ear = eye_aspect_ratio(left_eye)\n",
    "                right_eye_ear = eye_aspect_ratio(right_eye)\n",
    "                ear = (left_eye_ear + right_eye_ear) / 2.0\n",
    "\n",
    "                # Detect blink based on EAR threshold\n",
    "                if ear < EYE_AR_THRESH:\n",
    "                    blink_counter += 1\n",
    "                else:\n",
    "                    if blink_counter >= EYE_AR_CONSEC_FRAMES:\n",
    "                        blink_total += 1\n",
    "                        cheat_intensity += 1\n",
    "                    blink_counter = 0\n",
    "\n",
    "                # Display blink count\n",
    "                cv2.putText(image, f\"Blinks: {blink_total}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "\n",
    "                # Detect gaze direction\n",
    "                gaze_direction = get_gaze_direction(face_landmarks.landmark, image.shape)\n",
    "                cv2.putText(image, f\"Gaze: {gaze_direction}\", (10, 90), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 0), 2)\n",
    "\n",
    "                # Detect lip movement\n",
    "                upper_lip = face_landmarks.landmark[UPPER_LIP_INDEX]\n",
    "                lower_lip = face_landmarks.landmark[LOWER_LIP_INDEX]\n",
    "                lip_distance = abs(upper_lip.y - lower_lip.y)\n",
    "\n",
    "                if lip_distance > 0.05:\n",
    "                    if not lip_open:\n",
    "                        lip_open_count += 1\n",
    "                        lip_open = True\n",
    "                else:\n",
    "                    lip_open = False\n",
    "\n",
    "                cv2.putText(image, f\"Lip Opens: {lip_open_count}\", (10, 150), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 255), 2)\n",
    "\n",
    "                # Detect voice activity\n",
    "                if voice_detected:\n",
    "                    cv2.putText(image, \"Voice Detected!\", (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                if lip_open_count >= 3 and voice_detected:\n",
    "                    cv2.putText(image, \"Suspicious Activity Detected!\", (10, 300), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                \n",
    "                #########################\n",
    "                \n",
    "                # Calculate and display iris positions\n",
    "                mesh_points = np.array([np.multiply([p.x, p.y], [img_w, img_h]).astype(int) for p in face_landmarks.landmark])\n",
    "\n",
    "                (l_cx, l_cy), l_radius = cv2.minEnclosingCircle(mesh_points[LEFT_IRIS])\n",
    "                center_left = np.array([l_cx, l_cy], dtype=np.int32)\n",
    "                cv2.circle(image, center_left, int(l_radius), (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                (r_cx, r_cy), r_radius = cv2.minEnclosingCircle(mesh_points[RIGHT_IRIS])\n",
    "                center_right = np.array([r_cx, r_cy], dtype=np.int32)\n",
    "                cv2.circle(image, center_right, int(r_radius), (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "                left_eye_width = np.linalg.norm(left_eye[0] - left_eye[3])\n",
    "                right_eye_width = np.linalg.norm(right_eye[0] - right_eye[3])\n",
    "\n",
    "                left_gaze_ratio = (center_left[0] - left_eye[0][0]) / left_eye_width\n",
    "                right_gaze_ratio = (center_right[0] - right_eye[0][0]) / right_eye_width\n",
    "\n",
    "                iris_gaze_direction = \"Center\"\n",
    "                if left_gaze_ratio < 0.5 and right_gaze_ratio < 0.5:\n",
    "                    iris_gaze_direction = \"Looking Left\"\n",
    "                elif left_gaze_ratio > 0.5 and right_gaze_ratio > 0.5:\n",
    "                    iris_gaze_direction = \"Looking Right\"\n",
    "\n",
    "                cv2.putText(image, f'Iris Gaze: {iris_gaze_direction}', (10, 120), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                # Append the captured metrics to the lists\n",
    "                blink_data.append(blink_total)\n",
    "                voice_data.append(1 if voice_detected else 0)  # 1 if voice is detected, 0 otherwise\n",
    "                lip_open_data.append(lip_open_count)\n",
    "                \n",
    "                # Convert gaze direction into numerical data for plotting\n",
    "                gaze_direction_map = {\"head turned Left\": -1, \"Looking Forward\": 0, \"head turned Right\": 1}\n",
    "                gaze_data.append(gaze_direction_map.get(gaze_direction, 0))\n",
    "\n",
    "\n",
    "                # Collect system data\n",
    "                cpu_usage_data.append(psutil.cpu_percent())\n",
    "                memory_usage_data.append(psutil.virtual_memory().percent)\n",
    "                fps_data.append(1 / (time.time() - frame_start_time))\n",
    "                ######2\n",
    "                # Measure CPU and memory usage\n",
    "                cpu_usage = psutil.cpu_percent()\n",
    "                memory_usage = psutil.virtual_memory().percent\n",
    "                frame_end_time = time.time()\n",
    "                fps = 1 / (frame_end_time - frame_start_time)\n",
    "\n",
    "                # Store performance metrics\n",
    "                cpu_usage_data.append(cpu_usage)\n",
    "                memory_usage_data.append(memory_usage)\n",
    "                fps_data.append(fps)\n",
    "\n",
    "                # Display performance metrics\n",
    "                cv2.putText(image, f\"CPU: {cpu_usage:.2f}%\", (10, 210), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)\n",
    "                cv2.putText(image, f\"Memory: {memory_usage:.2f}%\", (10, 240), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)\n",
    "                cv2.putText(image, f\"FPS: {fps:.2f}\", (10, 270), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 0, 255), 2)\n",
    "\n",
    "                # Display the image\n",
    "                cv2.imshow('Gaze, Blink, Voice, Lip, and Iris Detector', image)\n",
    "\n",
    "                if cv2.waitKey(5) & 0xFF == 27:\n",
    "                    break\n",
    "                # Collect data for each frame\n",
    "                frame_info = {\n",
    "                    \"frame\": len(frame_data),\n",
    "                    \"cpu_usage\": cpu_usage,\n",
    "                    \"memory_usage\": memory_usage,\n",
    "                    \"fps\": fps,\n",
    "                    \"blink_total\": blink_total,\n",
    "                    \"voice_detected\": int(voice_detected),\n",
    "                    \"lip_open_count\": lip_open_count,\n",
    "                    \"gaze_direction\": gaze_direction_map.get(gaze_direction, 0),\n",
    "                    \"iris_gaze_direction\": 1 if iris_gaze_direction == \"Looking Right\" else -1 if iris_gaze_direction == \"Looking Left\" else 0,\n",
    "                    \"suspicious\": int(lip_open_count >= 3 and voice_detected),  # Example label for suspicious activity\n",
    "                }\n",
    "\n",
    "                # Append frame info to frame_data list\n",
    "                frame_data.append(frame_info)\n",
    "# Write collected data to a CSV file after the loop ends\n",
    "csv_file = \"suspicious_activity_data.csv\"\n",
    "csv_columns = [\"frame\", \"cpu_usage\", \"memory_usage\", \"fps\", \"blink_total\", \"voice_detected\", \"lip_open_count\", \"gaze_direction\", \"iris_gaze_direction\", \"suspicious\"]\n",
    "\n",
    "try:\n",
    "    with open(csv_file, 'w', newline='') as csvfile:\n",
    "        writer = csv.DictWriter(csvfile, fieldnames=csv_columns)\n",
    "        writer.writeheader()\n",
    "        for data in frame_data:\n",
    "            writer.writerow(data)\n",
    "    print(f\"Data saved to {csv_file}\")\n",
    "except IOError:\n",
    "    print(\"I/O error occurred while saving CSV\")\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7361e690",
   "metadata": {},
   "outputs": [],
   "source": [
    "Plot the performance and event data with Seaborn\n",
    "plt.figure(figsize=(14, 10))\n",
    "\n",
    "# CPU Usage Plot\n",
    "plt.subplot(3, 2, 1)\n",
    "sns.lineplot(x=range(len(cpu_usage_data)), y=cpu_usage_data, color='blue')\n",
    "plt.title(\"CPU Usage Over Time\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"CPU Usage (%)\")\n",
    "\n",
    "# Memory Usage Plot\n",
    "plt.subplot(3, 2, 2)\n",
    "sns.lineplot(x=range(len(memory_usage_data)), y=memory_usage_data, color='green')\n",
    "plt.title(\"Memory Usage Over Time\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Memory Usage (%)\")\n",
    "\n",
    "# FPS Plot\n",
    "plt.subplot(3, 2, 3)\n",
    "sns.lineplot(x=range(len(fps_data)), y=fps_data, color='red')\n",
    "plt.title(\"FPS Over Time\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Frames per Second (FPS)\")\n",
    "\n",
    "# Blink Count Plot\n",
    "plt.subplot(3, 2, 4)\n",
    "sns.lineplot(x=range(len(blink_data)), y=blink_data, color='blue')\n",
    "plt.title(\"Blink Count Over Time\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Total Blinks\")\n",
    "\n",
    "# Voice Detection Plot\n",
    "plt.subplot(3, 2, 5)\n",
    "sns.lineplot(x=range(len(voice_data)), y=voice_data, color='purple')\n",
    "plt.title(\"Voice Detection Over Time\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Voice Detected (1 = Yes, 0 = No)\")\n",
    "\n",
    "# Lip Opens Plot\n",
    "plt.subplot(3, 2, 6)\n",
    "sns.lineplot(x=range(len(lip_open_data)), y=lip_open_data, color='green')\n",
    "plt.title(\"Lip Opens Over Time\")\n",
    "plt.xlabel(\"Frames\")\n",
    "plt.ylabel(\"Total Lip Opens\")\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77c0759e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('whitegrid')\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ebab6df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('suspicious_activity_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0a77c020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>frame</th>\n",
       "      <th>cpu_usage</th>\n",
       "      <th>memory_usage</th>\n",
       "      <th>fps</th>\n",
       "      <th>blink_total</th>\n",
       "      <th>voice_detected</th>\n",
       "      <th>lip_open_count</th>\n",
       "      <th>gaze_direction</th>\n",
       "      <th>iris_gaze_direction</th>\n",
       "      <th>suspicious</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>267</th>\n",
       "      <td>267</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.3</td>\n",
       "      <td>55.411314</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>268</th>\n",
       "      <td>268</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>61.203911</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>269</th>\n",
       "      <td>269</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>60.049021</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>270</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>58.798929</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>271</th>\n",
       "      <td>271</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>55.512521</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>272</th>\n",
       "      <td>272</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>61.709073</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>273</th>\n",
       "      <td>273</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>54.033598</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>-1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>274</th>\n",
       "      <td>274</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>61.511784</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>275</th>\n",
       "      <td>275</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>60.892915</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>276</th>\n",
       "      <td>276</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>57.709985</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>277</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>58.810471</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>278</th>\n",
       "      <td>278</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>56.423590</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>279</th>\n",
       "      <td>279</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>59.875860</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>280</th>\n",
       "      <td>280</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>61.139675</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>281</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>53.619830</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>282</th>\n",
       "      <td>282</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>64.441501</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>283</th>\n",
       "      <td>283</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>55.893498</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>284</th>\n",
       "      <td>284</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>59.284287</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>285</th>\n",
       "      <td>285</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>55.093247</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>286</th>\n",
       "      <td>286</td>\n",
       "      <td>0.0</td>\n",
       "      <td>85.6</td>\n",
       "      <td>62.712000</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     frame  cpu_usage  memory_usage        fps  blink_total  voice_detected  \\\n",
       "267    267        0.0          85.3  55.411314            4               0   \n",
       "268    268        0.0          85.6  61.203911            4               0   \n",
       "269    269        0.0          85.6  60.049021            4               0   \n",
       "270    270        0.0          85.6  58.798929            4               0   \n",
       "271    271        0.0          85.6  55.512521            4               0   \n",
       "272    272        0.0          85.6  61.709073            4               0   \n",
       "273    273        0.0          85.6  54.033598            4               0   \n",
       "274    274        0.0          85.6  61.511784            4               0   \n",
       "275    275        0.0          85.6  60.892915            4               1   \n",
       "276    276        0.0          85.6  57.709985            4               1   \n",
       "277    277        0.0          85.6  58.810471            4               1   \n",
       "278    278        0.0          85.6  56.423590            4               1   \n",
       "279    279        0.0          85.6  59.875860            4               1   \n",
       "280    280        0.0          85.6  61.139675            4               1   \n",
       "281    281        0.0          85.6  53.619830            4               1   \n",
       "282    282        0.0          85.6  64.441501            4               1   \n",
       "283    283        0.0          85.6  55.893498            4               1   \n",
       "284    284        0.0          85.6  59.284287            4               1   \n",
       "285    285        0.0          85.6  55.093247            4               1   \n",
       "286    286        0.0          85.6  62.712000            4               1   \n",
       "\n",
       "     lip_open_count  gaze_direction  iris_gaze_direction  suspicious  \n",
       "267               5              -1                    1           0  \n",
       "268               5              -1                    1           0  \n",
       "269               5              -1                    1           0  \n",
       "270               5              -1                    1           0  \n",
       "271               5              -1                    1           0  \n",
       "272               5              -1                    1           0  \n",
       "273               5              -1                    1           0  \n",
       "274               5               0                    1           0  \n",
       "275               5               0                    1           1  \n",
       "276               5               0                    1           1  \n",
       "277               5               0                    1           1  \n",
       "278               5               0                    1           1  \n",
       "279               5               0                    1           1  \n",
       "280               5               0                    1           1  \n",
       "281               5               0                    1           1  \n",
       "282               5               0                    1           1  \n",
       "283               5               0                    1           1  \n",
       "284               5               0                    1           1  \n",
       "285               5               0                    1           1  \n",
       "286               5               0                    1           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail(20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a6f2c6d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/prathamhandique/Downloads/anaconda3/lib/python3.10/site-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.9557 - loss: 0.5307 - val_accuracy: 0.9565 - val_loss: 0.3162\n",
      "Epoch 2/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9341 - loss: 0.2651 - val_accuracy: 0.9565 - val_loss: 0.1479\n",
      "Epoch 3/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9866 - loss: 0.1162 - val_accuracy: 1.0000 - val_loss: 0.0707\n",
      "Epoch 4/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0557 - val_accuracy: 1.0000 - val_loss: 0.0353\n",
      "Epoch 5/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0322 - val_accuracy: 1.0000 - val_loss: 0.0201\n",
      "Epoch 6/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0163 - val_accuracy: 1.0000 - val_loss: 0.0131\n",
      "Epoch 7/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0181 - val_accuracy: 1.0000 - val_loss: 0.0091\n",
      "Epoch 8/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0088 - val_accuracy: 1.0000 - val_loss: 0.0069\n",
      "Epoch 9/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0071 - val_accuracy: 1.0000 - val_loss: 0.0054\n",
      "Epoch 10/10\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 1.0000 - loss: 0.0055 - val_accuracy: 1.0000 - val_loss: 0.0044\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 1.0000 - loss: 0.0019 \n",
      "Test Accuracy: 1.00\n",
      "\u001b[1m2/2\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv1D, Flatten\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "df = pd.read_csv('suspicious_activity_data.csv')\n",
    "\n",
    "# Separate features and target\n",
    "X = df.drop(columns=['suspicious']).values\n",
    "y = df['suspicious'].values\n",
    "\n",
    "# Normalize the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)\n",
    "\n",
    "# Reshape X to fit Conv1D input shape (samples, time steps, features)\n",
    "X = X.reshape((X.shape[0], X.shape[1], 1))\n",
    "\n",
    "# One-hot encode the target\n",
    "y = to_categorical(y, num_classes=2)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the CNN model\n",
    "model = Sequential([\n",
    "    Conv1D(64, kernel_size=2, activation='relu', input_shape=(X.shape[1], 1)),\n",
    "    Flatten(),\n",
    "    Dense(32, activation='relu'),\n",
    "    Dense(2, activation='softmax')  # 2 classes: suspicious and non-suspicious\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, batch_size=32, validation_split=0.1)\n",
    "\n",
    "# Evaluate the model\n",
    "loss, accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "# Predict on new data\n",
    "predictions = model.predict(X_test)\n",
    "predicted_classes = np.argmax(predictions, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ad0649",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
